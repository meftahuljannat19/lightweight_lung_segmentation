{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":258315,"sourceType":"datasetVersion","datasetId":108201},{"sourceId":8704049,"sourceType":"datasetVersion","datasetId":5220657},{"sourceId":10851259,"sourceType":"datasetVersion","datasetId":6739521},{"sourceId":10861686,"sourceType":"datasetVersion","datasetId":6747432},{"sourceId":10900229,"sourceType":"datasetVersion","datasetId":6774416},{"sourceId":11092388,"sourceType":"datasetVersion","datasetId":6914589}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"H = 256\nW = 256\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\nsmooth = 1.\ndef dice_coef(y_true, y_pred):\n    y_true_f = tf.keras.layers.Flatten()(y_true)\n    y_pred_f = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cbam_block(inputs, ratio=8):\n    channel_axis = -1\n    filters = inputs.shape[channel_axis]\n    se_shape = (1, 1, filters)\n\n    # Channel Attention\n    se = GlobalAveragePooling2D()(inputs)\n    se = Reshape(se_shape)(se)\n    se = Dense(filters // ratio, activation=tf.keras.layers.LeakyReLU(negative_slope=0.01), kernel_initializer='he_normal', use_bias=False)(se)\n    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n    x = Multiply()([inputs, se])\n\n    # Spatial Attention\n    se = Conv2D(filters // ratio, (1, 1), padding='same', \n                activation=tf.keras.layers.LeakyReLU(negative_slope=0.01), \n                kernel_initializer='he_normal', use_bias=False)(x)\n    se = Conv2D(1, (1, 1), padding='same', activation='sigmoid', \n                kernel_initializer='he_normal', use_bias=False)(se)\n    x = Multiply()([x, se])\n\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\n\ndef stem_block(x, n_filter, strides):\n    x_init = x\n\n    ## Conv 1\n    x = Conv2D(n_filter, (1, 1), padding=\"same\", strides=strides)(x_init)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = SeparableConv2D(n_filter, (3, 3), padding=\"same\")(x)\n    \n    #Conv 2\n    y = Conv2D(n_filter, (1, 1), padding=\"same\", strides=strides)(x_init)\n    \n    z = Conv2D(n_filter, (1, 1), padding=\"same\", strides=strides)(x_init)\n    z = BatchNormalization()(z)\n    z = Activation('relu')(z)\n    z = SeparableConv2D(n_filter, (5, 5), padding=\"same\")(z)\n\n    ## Shortcut\n    s = MaxPooling2D(pool_size=(3, 3),padding='same',strides=strides)(x_init)\n    s  = Conv2D(n_filter, (3, 3), padding=\"same\", strides=strides)(s)\n    \n\n    ## Add\n    x = Concatenate()([x,y,z,s])\n    x = cbam_block(x)\n    return x\n\ndef resnet_block(x, n_filter, strides=1):\n    x_init = x\n\n    ## Conv 1\n    x = BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n    x = SeparableConv2D(n_filter, (3, 3), padding=\"same\", strides=strides)(x)\n    ## Conv 2\n    x = BatchNormalization()(x)\n    x = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x)\n    x = SeparableConv2D(n_filter, (3, 3), padding=\"same\", strides=1)(x)\n\n    ## Shortcut\n    s  = Conv2D(n_filter, (1, 1), padding=\"same\", strides=strides)(x_init)\n    s = BatchNormalization()(s)\n\n    ## Add\n    x = Add()([x, s])\n    x = cbam_block(x)\n    return x\n\ndef aspp_block(x, num_filters, rate_scale=1):\n    x1 = Conv2D(num_filters, (3, 3), dilation_rate=(6 * rate_scale, 6 * rate_scale), padding=\"same\")(x)\n    x1 = BatchNormalization()(x1)\n\n    x2 = Conv2D(num_filters, (3, 3), dilation_rate=(12 * rate_scale, 12 * rate_scale), padding=\"same\")(x)\n    x2 = BatchNormalization()(x2)\n\n    x3 = Conv2D(num_filters, (3, 3), dilation_rate=(18 * rate_scale, 18 * rate_scale), padding=\"same\")(x)\n    x3 = BatchNormalization()(x3)\n\n    x4 = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n    x4 = BatchNormalization()(x4)\n\n    y = Add()([x1, x2, x3, x4])\n    y = Conv2D(num_filters, (1, 1), padding=\"same\")(y)\n    return y\n\ndef attetion_block(g, x):\n    \"\"\"\n        g: Output of Parallel Encoder block\n        x: Output of Previous Decoder block\n    \"\"\"\n\n    filters = x.shape[-1]\n\n    g_conv = BatchNormalization()(g)\n    g_conv = tf.keras.layers.LeakyReLU(negative_slope=0.01)(g_conv)\n    g_conv = Conv2D(filters, (3, 3), padding=\"same\")(g_conv)\n\n    g_pool = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(g_conv)\n\n    x_conv = BatchNormalization()(x)\n    x_conv = tf.keras.layers.LeakyReLU(negative_slope=0.01)(x_conv)\n    x_conv = Conv2D(filters, (3, 3), padding=\"same\")(x_conv)\n\n    gc_sum = Add()([g_pool, x_conv])\n\n    gc_conv = BatchNormalization()(gc_sum)\n    gc_conv = tf.keras.layers.LeakyReLU(negative_slope=0.01)(gc_conv)\n    gc_conv = Conv2D(filters, (3, 3), padding=\"same\")(gc_conv)\n\n    gc_mul = Multiply()([gc_conv, x])\n    return gc_mul\n\nclass LightResUnet:\n    def __init__(self, input_size=256):\n        self.input_size = input_size\n\n    def build_model(self):\n        n_filters = [16,32, 64, 128, 256]\n        inputs = Input((self.input_size, self.input_size, 3))\n\n        c0 = inputs\n        c1 = stem_block(c0, n_filters[0], strides=1)\n        print(c1.shape)\n        c2 = resnet_block(c1, n_filters[1], strides=2)\n        print(c2.shape)\n        c3 = resnet_block(c2, n_filters[2], strides=2)\n        print(c3.shape)\n        c4 = resnet_block(c3, n_filters[3], strides=2)\n        print(c4.shape)\n\n        b1 = aspp_block(c4, n_filters[4])\n\n        d1 = attetion_block(c3, b1)\n        d1 = UpSampling2D((2, 2))(d1)\n        d1 = Concatenate()([d1, c3])\n        d1 = resnet_block(d1, n_filters[3])\n\n        d2 = attetion_block(c2, d1)\n        d2 = UpSampling2D((2, 2))(d2)\n        d2 = Concatenate()([d2, c2])\n        d2 = resnet_block(d2, n_filters[2])\n\n        d3 = attetion_block(c1, d2)\n        d3 = UpSampling2D((2, 2))(d3)\n        d3 = Concatenate()([d3, c1])\n        d3 = resnet_block(d3, n_filters[1])\n\n        d4 = resnet_block(d3, n_filters[0])\n        outputs = Conv2D(1, (1, 1), padding=\"same\")(d3)\n        outputs = Activation(\"sigmoid\")(outputs)\n\n        model = Model(inputs, outputs)\n        return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arch = LightResUnet(input_size=256)\nmodel = arch.build_model()\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom glob import glob\nimport os\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Constants for image size\nW = 256\nH = 256\n\nimport os\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\n\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    return x\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x / 255.0\n    x = x.astype(np.float32)\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef tf_parse(image_path, mask_path):\n\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [H, W])\n    image = image / 255.0\n\n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=1)\n    mask = tf.image.resize(mask, [H, W])\n    mask = mask / 255.0\n\n    return image, mask\n\n\n\ndef tf_dataset(X, Y, batch=2, augment=False):\n    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n    dataset = dataset.map(tf_parse, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom glob import glob\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.metrics import Precision, Recall, MeanIoU, Accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\n\ndef load_dataset(path, num_data=1200, train_split=0.8, val_split=0.1, test_split=0.1):\n    images = sorted(glob(os.path.join(path, \"images\", \"*.jpg\")))[:num_data]\n    masks = sorted(glob(os.path.join(path, \"binary_masks\", \"*.png\")))[:num_data]\n\n    # Ensure that the splits add up to 1\n    assert train_split + val_split + test_split == 1, \"Splits must sum to 1.\"\n\n    # First, split the data into training and temporary set (valid + test)\n    train_x, temp_x, train_y, temp_y = train_test_split(images, masks, test_size=(1 - train_split), random_state=42)\n    \n    # Then, split the temporary set into validation and test sets\n    valid_x, test_x, valid_y, test_y = train_test_split(temp_x, temp_y, test_size=(test_split / (val_split + test_split)), random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)\n\ndataset_path = \"/kaggle/input/aquavplant-70-10-20-aug/train_augmented\"\n(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path, num_data=1100, train_split=0.8, val_split=0.1,test_split=0.1)\n\nprint(f\"Train: {len(train_x)} - {len(train_y)}\")\nprint(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\nprint(f\"Valid: {len(test_x)} - {len(test_y)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    create_dir(\"files\")\n\n\n    batch_size = 32\n    lr = 1e-3\n    num_epochs = 50\n    model_path = os.path.join(\"files\", \"resunet_model.keras\")\n    csv_path = os.path.join(\"files\", \"log.csv\")\n    \n    train_dataset = tf_dataset(train_x, train_y, batch=batch_size,augment=False)\n    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size,augment=False)\n    test_dataset = tf_dataset(test_x, test_y, batch=batch_size, augment= False)\n    \n    plt.figure(figsize=(15, 6))\n    for i, (image, mask) in enumerate(train_dataset.take(1)):\n        for j in range(10):\n            plt.subplot(2, 10, j + 1)\n            plt.imshow(image[j])\n            plt.title('Image')\n            plt.axis('off')\n\n            plt.subplot(2, 10, j + 11)\n            plt.imshow(mask[j][:, :, 0], cmap='gray')\n            plt.title('Mask')\n            plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n    metrics = [Recall(), Precision(), dice_coef, MeanIoU(num_classes=2)]\n    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n\n    callbacks = [\n        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-10, verbose=1),\n        CSVLogger(csv_path),\n        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False),\n    ]\n\n    history = model.fit(\n        train_dataset,\n        epochs=num_epochs,\n        validation_data=valid_dataset,\n        callbacks=callbacks\n    )\n        \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.utils import CustomObjectScope\nfrom sklearn.metrics import f1_score, jaccard_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.utils import CustomObjectScope\nfrom sklearn.metrics import f1_score, jaccard_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, Reshape, multiply, Add, Activation, Concatenate, Conv2D, Layer\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom sklearn.metrics import confusion_matrix   \nif __name__ == \"__main__\":\n    np.random.seed(42)\n    tf.random.set_seed(42)\n\n    with CustomObjectScope({\"dice_coef\": dice_coef, \"dice_loss\": dice_loss, \n                            'LeakyReLU':tf.keras.layers.LeakyReLU(negative_slope=0.01),\n                            'cbam_block':cbam_block}):\n        model = tf.keras.models.load_model(\"/kaggle/working/files/resunet_model.keras\")\n\n    SCORE = []\n    image_data = []\n    for x, y in tqdm(zip(test_x, test_y), total=len(test_y)):\n        \"\"\" Extracting the name \"\"\"\n        name = x.split(\"/\")[-1]\n\n        \"\"\" Reading the image \"\"\"\n        image = cv2.imread(x, cv2.IMREAD_COLOR) \n        image = cv2.resize(image, (W, H))       \n        x = image / 255.0                       \n        x = np.expand_dims(x, axis=0)           \n\n        \"\"\" Reading the mask \"\"\"\n        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.resize(mask, (W, H))\n        mask = mask/255.0\n\n        \"\"\" Prediction \"\"\"\n        y_pred = model.predict(x, verbose=0)[0]\n        y_pred = np.squeeze(y_pred, axis=-1)\n        #print(\"Min/Max Predicted Mask:\", y_pred.min(), y_pred.max())  # Debugging\n\n        y_pred = (y_pred > 0.5).astype(np.int32)  # Use `>` instead of `>=`\n\n        \"\"\" Flatten the arrays \"\"\"\n        mask = (mask > 0).astype(np.int32).flatten()\n        y_pred = y_pred.flatten()\n\n        \"\"\" Compute TP, TN, FP, FN manually \"\"\"\n        TP = np.sum((mask == 1) & (y_pred == 1))\n        TN = np.sum((mask == 0) & (y_pred == 0))\n        FP = np.sum((mask == 0) & (y_pred == 1))\n        FN = np.sum((mask == 1) & (y_pred == 0))\n\n        \"\"\" Calculate metrics manually \"\"\"\n        accuracy_value = (TP + TN) / (TP + TN + FP + FN)\n        precision_value = TP / (TP + FP) if (TP + FP) > 0 else 0\n        recall_value = TP / (TP + FN) if (TP + FN) > 0 else 0\n        specificity_value = TN / (TN + FP) if (TN + FP) > 0 else 0\n        dice_value = (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n        jac_value = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n        f1_value = (2 * precision_value * recall_value) / (precision_value + recall_value) if (precision_value + recall_value) > 0 else 0\n\n        \"\"\" Store the results \"\"\"\n        SCORE.append([name, accuracy_value, f1_value, jac_value, recall_value, precision_value, specificity_value, dice_value])\n\n    \"\"\" Compute average scores \"\"\"\n    score = np.mean([s[1:] for s in SCORE], axis=0)\n\n    print(f\"Accuracy: {score[0]:0.5f}\")\n    print(f\"F1: {score[1]:0.5f}\")\n    print(f\"Jaccard: {score[2]:0.5f}\")\n    print(f\"Recall: {score[3]:0.5f}\")\n    print(f\"Precision: {score[4]:0.5f}\")\n    print(f\"Specificity: {score[5]:0.5f}\")\n    print(f\"Dice: {score[6]:0.5f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_results(image, mask, prediction, name):\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title('Image')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 2)\n    plt.imshow(mask, cmap='gray')\n    plt.title('Mask')\n    plt.axis('off')\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(prediction, cmap='gray')\n    plt.title('Prediction')\n    plt.axis('off')\n\n    plt.suptitle(name)\n    plt.show()\n\nimport matplotlib.pyplot as plt\nlast_image_data = image_data[10]\nif last_image_data is not None:\n  plot_results(*last_image_data)\nelse:\n  print(\"No image data found.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/files/score.csv\")\ndf.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/files/log.csv\")\ndf.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}